# ----------------------------------------------------------------------------
# Copyright (c) 2021-2025 DexForce Technology Co., Ltd.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ----------------------------------------------------------------------------
import os
import re
import torch
import numpy as np
from embodichain.lab.gym.envs import EmbodiedEnv, EmbodiedEnvCfg
from embodichain.lab.gym.utils.registration import register_env
from embodichain.lab.sim.utility.action_utils import interpolate_with_distance_warp
from embodichain.utils import logger
from embodichain.lab.sim.objects import Robot
import json
import cv2 as cv
from typing import Tuple, Dict
from dexsim.types import PhysicalAttr, ActorType
from embodichain.lab.sim.types import EnvObs, EnvAction
from copy import deepcopy

__all__ = ["LerobotGenRawEnv"]

# TODO: move to cfg file
RAW_DATA_DIR = "/home/chenjian/Downloads/sack_carton_raw_data_bk"
PLACE_POSITION = [0.0, 0.5, 0.25]


@register_env("LerobotGenRaw", max_episode_steps=600)
class LerobotGenRawEnv(EmbodiedEnv):
    def __init__(self, cfg: EmbodiedEnvCfg = None, **kwargs):
        self.episode_counter = -1

        super().__init__(cfg, **kwargs)

        config = {"data_dir": RAW_DATA_DIR, "device": "cpu", "headless": True}
        self.converter = ConvertToLeRobot(**config)
        self.converter.set_robot(self.robot)
        self.trajectory_list, self.grasp_list, self.rgb_list, self.depth_list = (
            self._create_full_trajectory()
        )

        self._has_pick = False
        self._has_drop = False

    def _create_full_trajectory(self):
        # TODO: read these from cfg file
        # need depth image to get grasp position

        # save multiple trajectorys in traj_dict
        trajectory_dict = self.converter.generate_grasp_trajectory()
        trajectory_list = []
        rgb_list = []
        depth_list = []
        grasp_list = []
        sensor = self.sim.get_sensor("camera_1")
        # img_size = (sensor.cfg.height, sensor.cfg.width)
        img_size = (sensor.cfg.width, sensor.cfg.height)
        for key, traj in trajectory_dict.items():
            n_traj = len(traj["traj_list"])
            trajectory_list.extend(traj["traj_list"])
            grasp_list.extend(traj["grasp_poses"])
            rgb_path = self.converter.file_dict[key]["rgb_path"]
            rgb_np = cv.imread(rgb_path, cv.IMREAD_UNCHANGED)
            rgb_resize = cv.resize(rgb_np, img_size, interpolation=cv.INTER_LINEAR)
            depth_path = self.converter.file_dict[key]["depth_path"]
            depth_np = cv.imread(depth_path, cv.IMREAD_UNCHANGED)
            depth_resize = cv.resize(depth_np, img_size, interpolation=cv.INTER_NEAREST)
            rgb_single_list = [rgb_resize for i in range(n_traj)]
            depth_single_list = [depth_resize for i in range(n_traj)]
            rgb_list.extend(rgb_single_list)
            depth_list.extend(depth_single_list)
        self.n_trajectory = len(trajectory_list)
        return trajectory_list, grasp_list, rgb_list, depth_list

    def create_demo_action_list(self, *args, **kwargs):
        """
        Create a demonstration action list for the current task.

        Returns:
            list: A list of demo actions generated by the task.
        """
        self.episode_counter += 1
        episode_id = self.episode_counter % len(self.trajectory_list)
        trajectory = self.trajectory_list[episode_id]

        # reset grasp object pose
        grasp_pose = self.grasp_list[episode_id]
        grasp_object = self.sim.get_rigid_object("grasp_object")
        for entity in grasp_object._entities:
            entity.set_actor_type(ActorType.KINEMATIC)
        grasp_object_aabb = grasp_object._entities[0].get_aabb_attr()
        current_world_pose = grasp_object._entities[0].get_world_pose()
        z_shift = current_world_pose[2, 3] - grasp_object_aabb[5]

        grasp_pose[:3, :3] = np.eye(3)  # ignore rotation for now
        grasp_pose[2, 3] += z_shift
        n_envs = self.robot.num_instances
        grasp_pose_t = torch.tensor(
            grasp_pose, device=self.robot.device, dtype=torch.float32
        )
        grasp_pose_t = grasp_pose_t[None, :, :].repeat(n_envs, 1, 1)
        grasp_object.set_local_pose(grasp_pose_t)

        self._has_pick = False
        self._has_drop = False

        trajectory = trajectory[:, None, :].repeat(1, n_envs, 1)
        return trajectory

    def get_obs(self, **kwargs) -> EnvObs:
        obs = super().get_obs()
        n_env = self.sim.num_envs
        if self.episode_counter < 0:
            return obs
        else:
            episode_id = (self.episode_counter - 1) % len(self.rgb_list)
            rgb = torch.tensor(
                self.rgb_list[episode_id], device=self.robot.device, dtype=torch.uint8
            )[None, :, :, :].repeat(n_env, 1, 1, 1)
            depth = torch.tensor(
                self.depth_list[episode_id],
                device=self.robot.device,
                dtype=torch.float32,
            )[None, :, :].repeat(n_env, 1, 1)
        obs["sensor"]["camera_1"]["color"] = rgb
        obs["sensor"]["camera_1"]["depth"] = depth
        return obs

    def _update_sim_state(self, **kwargs) -> None:
        end_xpos = (
            self.robot.compute_fk(
                qpos=self.robot.get_qpos(), name="arm", to_matrix=True
            )[0]
            .to("cpu")
            .numpy()
        )
        grasp_object = self.sim.get_rigid_object("grasp_object")
        grasp_entity0 = grasp_object._entities[0]
        robot_entity0 = self.robot._entities[0]

        grasp_object_aabb = grasp_entity0.get_aabb_attr()
        grasp_pose = grasp_entity0.get_world_pose()
        grasp_position = deepcopy(grasp_pose[:3, 3])
        grasp_position[2] = grasp_object_aabb[5]  # to top surface

        obj_distance = np.linalg.norm(end_xpos[:3, 3] - grasp_position)
        if obj_distance < 0.06 and not self._has_pick and not self._has_drop:
            # object move with ee_link
            end_link_pose = robot_entity0.get_link_pose("ee_link")
            self.obj_relative_pose = inv_transform(end_link_pose) @ grasp_pose
            self.obj_relative_pose = torch.tensor(
                self.obj_relative_pose, dtype=torch.float32, device=self.robot.device
            )
            self._has_pick = True

        place_distance = np.linalg.norm(end_xpos[:3, 3] - np.array(PLACE_POSITION))
        if place_distance < 0.07 and self._has_pick:
            self._has_pick = False
            self._has_drop = True
            for entity in grasp_object._entities:
                entity.set_actor_type(ActorType.DYNAMIC)

        if self._has_pick:
            end_link_pose = self.robot.get_link_pose("ee_link", to_matrix=True)
            current_obj_pose = grasp_object.get_local_pose(to_matrix=True)
            n_env = current_obj_pose.shape[0]
            for i in range(n_env):
                current_obj_pose[i] = end_link_pose[i] @ self.obj_relative_pose
            grasp_object.set_local_pose(current_obj_pose)


def _bilinear_sample(
    image: np.ndarray, u: np.ndarray, v: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """Bilinear sample `image` at float coordinates (u, v).

    Returns (values, valid_mask) where valid_mask is False for coordinates outside the image.
    image is indexed as image[v, u] (row, col).
    """
    H, W = image.shape[:2]

    u = np.asarray(u, dtype=float)
    v = np.asarray(v, dtype=float)

    x0 = np.floor(u).astype(int)
    x1 = x0 + 1
    y0 = np.floor(v).astype(int)
    y1 = y0 + 1

    # valid if all four neighbours are inside
    valid = (x0 >= 0) & (y0 >= 0) & (x1 < W) & (y1 < H)

    # clip indices to safe range for indexing (we'll mask invalid later)
    x0c = np.clip(x0, 0, W - 1)
    x1c = np.clip(x1, 0, W - 1)
    y0c = np.clip(y0, 0, H - 1)
    y1c = np.clip(y1, 0, H - 1)

    Ia = image[y0c, x0c]
    Ib = image[y0c, x1c]
    Ic = image[y1c, x0c]
    Id = image[y1c, x1c]

    wa = (x1 - u) * (y1 - v)
    wb = (u - x0) * (y1 - v)
    wc = (x1 - u) * (v - y0)
    wd = (u - x0) * (v - y0)

    values = wa * Ia + wb * Ib + wc * Ic + wd * Id

    # for points with invalid neighbours, mark invalid and set value 0
    values = np.where(valid, values, 0.0)

    return values, valid


def _nearest_sample(
    image: np.ndarray, u: np.ndarray, v: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    H, W = image.shape[:2]
    ui = np.rint(u).astype(int)
    vi = np.rint(v).astype(int)
    valid = (ui >= 0) & (vi >= 0) & (ui < W) & (vi < H)
    vals = np.zeros(u.shape, dtype=image.dtype)
    inds = np.where(valid)
    if inds[0].size:
        vals[inds] = image[vi[inds], ui[inds]]
    return vals, valid


def inv_transform(transform: np.ndarray) -> np.ndarray:
    """inverse transformation

    Args:
        transform (np.array): [np.array of size [4 x 4]]

    Returns:
        np.array: [np.array of size [4 x 4]]
    """
    r = transform[:3, :3]
    t = transform[:3, 3].T
    inv_r = r.T
    inv_t = -inv_r @ t
    inv_pose = np.eye(4, dtype=np.float32)
    inv_pose[:3, :3] = inv_r
    inv_pose[:3, 3] = inv_t
    return inv_pose


def pixels_to_pointcloud(
    depth_image: np.ndarray,
    pixels: np.ndarray,
    K: np.ndarray,
    sampling: str = "bilinear",
) -> Tuple[np.ndarray, np.ndarray]:
    """Convert image-space pixel coordinates + depth image to 3D points (camera frame).

    Args:
        depth_image: (H, W) depth array (float)
        pixels: (N, 2) float array of pixel coordinates (u, v) where u is column and v is row
        K: (3, 3) camera intrinsic matrix
        sampling: 'bilinear' or 'nearest'

    Returns:
        points: (N, 3) float array of 3D camera-frame points. For invalid samples values are 0.

    Notes:
    - Uses convention image[v, u] for accessing depth_image. Pixel coords are zero-indexed.
    - Points are computed as X = z * K^{-1} * [u, v, 1]^T
    """
    depth_image = np.asarray(depth_image)
    pixels = np.asarray(pixels, dtype=float)
    K = np.asarray(K, dtype=float)

    if pixels.ndim != 2 or pixels.shape[1] != 2:
        raise ValueError("pixels must be an (N,2) array of (u, v) coordinates")

    u = pixels[:, 0]
    v = pixels[:, 1]
    N = pixels.shape[0]

    if sampling == "bilinear":
        z, valid = _bilinear_sample(depth_image, u, v)
    elif sampling == "nearest":
        z, valid = _nearest_sample(depth_image, u, v)
    else:
        raise ValueError("sampling must be 'bilinear' or 'nearest'")
    # require depth > 0.2 and <= 10.0
    valid = valid & (z > 0.2) & (z <= 10.0)

    # If everything is invalid, caller requested we 'skip' â€” return empty arrays to signal skip.
    # Assumption: 'skip' means return zero-length point array and zero-length mask when return_mask=True.
    if np.all(~valid):
        return np.empty((0, 3), dtype=float)

    # build homogeneous pixel coordinates
    ones = np.ones((N,), dtype=float)
    uv1 = np.stack([u, v, ones], axis=1)  # (N,3)

    # invert intrinsics
    K_inv = np.linalg.inv(K)

    # directions = K_inv @ uv1.T -> (3, N) then transpose
    dirs = (K_inv @ uv1.T).T  # (N,3)

    points = dirs * z[:, None]

    # set invalid points to zeros (and optionally mask will indicate)
    valid_points = points[valid]
    return valid_points


class ConvertToLeRobot:
    def __init__(self, **kwargs):
        self.config = kwargs
        self.file_dict = self._get_file_dict(self.config.get("data_dir", None))

    def set_robot(self, robot: Robot):
        self.robot = robot
        self.init_qpos = robot.get_qpos()
        self.init_xpos = (
            robot.compute_fk(qpos=self.init_qpos, name="arm", to_matrix=True)[0]
            .to("cpu")
            .numpy()
        )
        self.grasp_position_dict = self._get_grasp_position_in_cam()

    def _get_file_dict(self, data_dir: str):
        # get all unique pattern
        uid_pattern = re.compile(r"^[a-f0-9]{32}$")
        files = os.listdir(data_dir)
        uid_set = set()
        for f in files:
            base_name, ext = os.path.splitext(f)
            if uid_pattern.match(base_name):
                uid_set.add(base_name)
        uid_list = list(uid_set)

        # convert to file dict
        file_dict = {}
        for uid in uid_list:
            depth_path = os.path.join(data_dir, f"{uid}.tiff")
            rgb_path = os.path.join(data_dir, f"{uid}.jpg")
            json_path = os.path.join(data_dir, f"{uid}.json")
            if not os.path.isfile(depth_path) or not os.path.isfile(json_path):
                continue
            file_dict[uid] = {
                "depth_path": depth_path,
                "rgb_path": rgb_path,
                "json_path": json_path,
            }
        return file_dict

    def _get_grasp_position_in_cam(self):
        # TODO: we can add random perturbation to grasp position
        grasp_position_dict = {}
        for uid, paths in self.file_dict.items():
            config = json.load(open(paths["json_path"], "r"))
            cam_intrinsic = np.array(config["images"][0]["K"]).reshape(3, 3)
            depth_img = cv.imread(paths["depth_path"], cv.IMREAD_UNCHANGED)
            grasp_position_list = []
            for annotation in config["annotations"]:
                segmentation = annotation["segmentation"]
                for single_seg in segmentation:
                    float_pixel_arr = np.array(single_seg).reshape(-1, 2)
                    if float_pixel_arr.shape[0] < 4:
                        continue
                    pc = pixels_to_pointcloud(
                        depth_image=depth_img,
                        pixels=float_pixel_arr,
                        K=cam_intrinsic,
                    )
                    is_nan_mask = np.isnan(pc).any(axis=1)
                    pc = pc[~is_nan_mask]
                    if pc.shape[0] < 4:
                        continue
                    pick_center_camera = np.mean(pc, axis=0)
                    grasp_position_list.append(pick_center_camera)
            grasp_arr = np.array(grasp_position_list)
            grasp_position_dict[uid] = grasp_arr
        return grasp_position_dict

    def generate_grasp_trajectory(self):
        grasp_positions = np.zeros(shape=(0, 3), dtype=np.float32)
        start_end_id_dict = {}
        for key, value in self.grasp_position_dict.items():
            start_id = grasp_positions.shape[0]
            end_id = start_id + value.shape[0]
            grasp_positions = np.vstack((grasp_positions, value))
            start_end_id_dict[key] = (start_id, end_id)
        grasp_poses = self.generate_grasp_poses(grasp_positions=grasp_positions)
        n_grasp = grasp_poses.shape[0]
        n_env = self.robot.num_instances
        # [n_grasp, n_waypoints, dof]
        dof = self.robot.dof
        success, waypoints = self.generate_grasp_waypoints(grasp_poses=grasp_poses)
        is_valid_arr = success.to("cpu").numpy()
        # do interpolation

        # [n_grasp, n_interp, dof]
        interp_trajectory = interpolate_with_distance_warp(
            trajectory=waypoints, interp_num=100, device=self.robot.device
        )
        # save successful result to dict
        trajectory_dict = {}
        for key, (start_id, end_id) in start_end_id_dict.items():
            info_dict = {"traj_list": [], "grasp_poses": []}
            for id in range(start_id, end_id):
                is_valid = is_valid_arr[id]
                if not is_valid:
                    continue
                info_dict["traj_list"].append(interp_trajectory[id])
                info_dict["grasp_poses"].append(grasp_poses[id])
            trajectory_dict[key] = info_dict
        return trajectory_dict

    def generate_grasp_poses(self, grasp_positions: np.ndarray):
        # move all grasp center near init_xpos
        work_center = self.init_xpos[:3, 3]
        work_center[2] -= 0.6
        grasp_center = grasp_positions.mean(axis=0)
        grasp_posi_translated = grasp_positions - grasp_center + work_center

        # get grasp poses
        rotation = self.init_xpos[:3, :3]
        n_grasp = grasp_posi_translated.shape[0]
        grasp_poses = np.zeros((n_grasp, 4, 4), dtype=np.float32)
        for i in range(n_grasp):
            # TODO: we can add random rotation around z axis
            grasp_poses[i, :3, :3] = rotation
            grasp_poses[i, :3, 3] = grasp_posi_translated[i]
            grasp_poses[i, 3, 3] = 1.0
        return grasp_poses

    def generate_grasp_waypoints(self, grasp_poses: np.ndarray):
        n_grasp = grasp_poses.shape[0]
        n_envs = self.robot.num_instances
        # get xpos waypoints
        grasp_poses = torch.tensor(
            grasp_poses, device=self.robot.device, dtype=torch.float32
        )  # [n_grasp, 4, 4]

        # place poses
        place_pose = deepcopy(grasp_poses[0])
        place_pose[:3, 3] = torch.tensor(
            PLACE_POSITION, device=self.robot.device, dtype=torch.float32
        )
        place_poses = place_pose[None, :, :].repeat(n_grasp, 1, 1)

        # filter invalid grasp poses
        grasp_z = grasp_poses[:, 2, 3]
        valid_z_mask = torch.logical_and(
            grasp_z > 0.2, grasp_z < 0.7
        )  # TODO: hard coded

        approch_poses = grasp_poses.clone()
        approch_poses[:, 2, 3] += 0.18  # TODO: Hard coded. Approch 10cm above
        batch_init_qpos = self.init_qpos[0, None, :].repeat(n_grasp, 1)
        approach_place_poses = place_poses.clone()
        approach_place_poses[:, 2, 3] += 0.3

        approach_success, approach_qpos = self.temp_compute_batch_ik(
            approch_poses, batch_init_qpos
        )
        grasp_success, grasp_qpos = self.temp_compute_batch_ik(
            grasp_poses, approach_qpos
        )

        approach_place_success, approach_place_qpos = self.temp_compute_batch_ik(
            approach_place_poses, approach_qpos
        )

        place_success, place_qpos = self.temp_compute_batch_ik(
            place_poses, approach_place_qpos
        )

        # pack results
        success = torch.logical_and(approach_success, grasp_success)
        success = torch.logical_and(success, approach_place_success)
        success = torch.logical_and(success, place_success)
        success = torch.logical_and(success, valid_z_mask)
        waypoints = torch.concatenate(
            [
                batch_init_qpos[:, None, :],
                approach_qpos[:, None, :],
                grasp_qpos[:, None, :],
                approach_qpos[:, None, :],
                approach_place_qpos[:, None, :],
                place_qpos[:, None, :],
                approach_place_qpos[:, None, :],
                batch_init_qpos[:, None, :],
            ],
            dim=1,
        )  # [n_grasp, n_waypoints, dof]

        return success, waypoints

    def temp_compute_batch_ik(self, xpos: torch.Tensor, joint_seed: torch.Tensor):
        n_grasp = xpos.shape[0]
        is_success = torch.zeros((n_grasp,), dtype=torch.bool, device=self.robot.device)
        qpos = torch.zeros(
            (n_grasp, self.robot.dof), dtype=torch.float32, device=self.robot.device
        )
        solver = self.robot._solvers["arm"]
        for i in range(n_grasp):
            is_success_i, qpos_i = solver.get_ik(
                target_xpos=xpos[i, :, :],
                qpos_seed=joint_seed[i, :],
                return_all_solutions=False,
            )
            is_success[i] = is_success_i
            qpos[i, :] = qpos_i.reshape(-1)

        return is_success, qpos

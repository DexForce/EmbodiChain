# ----------------------------------------------------------------------------
# Copyright (c) 2021-2025 DexForce Technology Co., Ltd.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ----------------------------------------------------------------------------
import os
import re
import torch
import numpy as np
from embodichain.lab.gym.envs import EmbodiedEnv, EmbodiedEnvCfg
from embodichain.lab.gym.utils.registration import register_env
from embodichain.lab.sim.utility.action_utils import interpolate_with_distance_warp
from embodichain.utils import logger
from embodichain.lab.sim.objects import Robot
import json
import cv2 as cv
from typing import Tuple, Dict

__all__ = ["LerobotGenEnv"]

# TODO: move to cfg file
RAW_DATA_DIR = "/home/chenjian/Downloads/sack_carton_raw_data_bk"


@register_env("LerobotGen", max_episode_steps=600)
class LerobotGenEnv(EmbodiedEnv):
    def __init__(self, cfg: EmbodiedEnvCfg = None, **kwargs):
        super().__init__(cfg, **kwargs)

        self.trajectory_list = self._create_full_trajectory()
        self.episode_counter = 0

    def _create_full_trajectory(self):
        # TODO: read these from cfg file
        # need depth image to get grasp position
        config = {"data_dir": RAW_DATA_DIR, "device": "cpu", "headless": True}
        converter = ConvertToLeRobot(robot=self.robot, **config)

        # save multiple trajectorys in traj_dict
        trajectory_dict = converter.generate_grasp_trajectory()
        trajectory_list = []
        for key, traj in trajectory_dict.items():
            trajectory_list.extend(traj)

        return trajectory_list

    def create_demo_action_list(self, *args, **kwargs):
        """
        Create a demonstration action list for the current task.

        Returns:
            list: A list of demo actions generated by the task.
        """
        trajectory = self.trajectory_list[
            self.episode_counter % len(self.trajectory_list)
        ]
        self.episode_counter += 1
        return trajectory[:, None, :]


def _bilinear_sample(
    image: np.ndarray, u: np.ndarray, v: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """Bilinear sample `image` at float coordinates (u, v).

    Returns (values, valid_mask) where valid_mask is False for coordinates outside the image.
    image is indexed as image[v, u] (row, col).
    """
    H, W = image.shape[:2]

    u = np.asarray(u, dtype=float)
    v = np.asarray(v, dtype=float)

    x0 = np.floor(u).astype(int)
    x1 = x0 + 1
    y0 = np.floor(v).astype(int)
    y1 = y0 + 1

    # valid if all four neighbours are inside
    valid = (x0 >= 0) & (y0 >= 0) & (x1 < W) & (y1 < H)

    # clip indices to safe range for indexing (we'll mask invalid later)
    x0c = np.clip(x0, 0, W - 1)
    x1c = np.clip(x1, 0, W - 1)
    y0c = np.clip(y0, 0, H - 1)
    y1c = np.clip(y1, 0, H - 1)

    Ia = image[y0c, x0c]
    Ib = image[y0c, x1c]
    Ic = image[y1c, x0c]
    Id = image[y1c, x1c]

    wa = (x1 - u) * (y1 - v)
    wb = (u - x0) * (y1 - v)
    wc = (x1 - u) * (v - y0)
    wd = (u - x0) * (v - y0)

    values = wa * Ia + wb * Ib + wc * Ic + wd * Id

    # for points with invalid neighbours, mark invalid and set value 0
    values = np.where(valid, values, 0.0)

    return values, valid


def _nearest_sample(
    image: np.ndarray, u: np.ndarray, v: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    H, W = image.shape[:2]
    ui = np.rint(u).astype(int)
    vi = np.rint(v).astype(int)
    valid = (ui >= 0) & (vi >= 0) & (ui < W) & (vi < H)
    vals = np.zeros(u.shape, dtype=image.dtype)
    inds = np.where(valid)
    if inds[0].size:
        vals[inds] = image[vi[inds], ui[inds]]
    return vals, valid


def pixels_to_pointcloud(
    depth_image: np.ndarray,
    pixels: np.ndarray,
    K: np.ndarray,
    sampling: str = "bilinear",
) -> Tuple[np.ndarray, np.ndarray]:
    """Convert image-space pixel coordinates + depth image to 3D points (camera frame).

    Args:
        depth_image: (H, W) depth array (float)
        pixels: (N, 2) float array of pixel coordinates (u, v) where u is column and v is row
        K: (3, 3) camera intrinsic matrix
        sampling: 'bilinear' or 'nearest'

    Returns:
        points: (N, 3) float array of 3D camera-frame points. For invalid samples values are 0.

    Notes:
    - Uses convention image[v, u] for accessing depth_image. Pixel coords are zero-indexed.
    - Points are computed as X = z * K^{-1} * [u, v, 1]^T
    """
    depth_image = np.asarray(depth_image)
    pixels = np.asarray(pixels, dtype=float)
    K = np.asarray(K, dtype=float)

    if pixels.ndim != 2 or pixels.shape[1] != 2:
        raise ValueError("pixels must be an (N,2) array of (u, v) coordinates")

    u = pixels[:, 0]
    v = pixels[:, 1]
    N = pixels.shape[0]

    if sampling == "bilinear":
        z, valid = _bilinear_sample(depth_image, u, v)
    elif sampling == "nearest":
        z, valid = _nearest_sample(depth_image, u, v)
    else:
        raise ValueError("sampling must be 'bilinear' or 'nearest'")
    # require depth > 0.2 and <= 10.0
    valid = valid & (z > 0.2) & (z <= 10.0)

    # If everything is invalid, caller requested we 'skip' â€” return empty arrays to signal skip.
    # Assumption: 'skip' means return zero-length point array and zero-length mask when return_mask=True.
    if np.all(~valid):
        return np.empty((0, 3), dtype=float)

    # build homogeneous pixel coordinates
    ones = np.ones((N,), dtype=float)
    uv1 = np.stack([u, v, ones], axis=1)  # (N,3)

    # invert intrinsics
    K_inv = np.linalg.inv(K)

    # directions = K_inv @ uv1.T -> (3, N) then transpose
    dirs = (K_inv @ uv1.T).T  # (N,3)

    points = dirs * z[:, None]

    # set invalid points to zeros (and optionally mask will indicate)
    valid_points = points[valid]
    return valid_points


class ConvertToLeRobot:
    def __init__(self, robot: Robot, **kwargs):
        self.config = kwargs
        self.robot = robot
        self.init_qpos = robot.get_qpos()
        self.init_xpos = (
            robot.compute_fk(qpos=self.init_qpos, name="arm", to_matrix=True)[0]
            .to("cpu")
            .numpy()
        )

        self.file_dict = self._get_file_dict(self.config.get("data_dir", None))
        self.grasp_position_dict = self._get_grasp_position_in_cam()

    def _get_file_dict(self, data_dir: str):
        # get all unique pattern
        uid_pattern = re.compile(r"^[a-f0-9]{32}$")
        files = os.listdir(data_dir)
        uid_set = set()
        for f in files:
            base_name, ext = os.path.splitext(f)
            if uid_pattern.match(base_name):
                uid_set.add(base_name)
        uid_list = list(uid_set)

        # convert to file dict
        file_dict = {}
        for uid in uid_list:
            depth_path = os.path.join(data_dir, f"{uid}.tiff")
            rgb_path = os.path.join(data_dir, f"{uid}.rgb")
            json_path = os.path.join(data_dir, f"{uid}.json")
            if not os.path.isfile(depth_path) or not os.path.isfile(json_path):
                continue
            file_dict[uid] = {
                "depth_path": depth_path,
                "rgb_path": rgb_path,
                "json_path": json_path,
            }
        return file_dict

    def _get_grasp_position_in_cam(self):
        # TODO: we can add random perturbation to grasp position
        grasp_position_dict = {}
        for uid, paths in self.file_dict.items():
            config = json.load(open(paths["json_path"], "r"))
            cam_intrinsic = np.array(config["images"][0]["K"]).reshape(3, 3)
            depth_img = cv.imread(paths["depth_path"], cv.IMREAD_UNCHANGED)
            grasp_position_list = []
            for annotation in config["annotations"]:
                segmentation = annotation["segmentation"]
                for single_seg in segmentation:
                    float_pixel_arr = np.array(single_seg).reshape(-1, 2)
                    if float_pixel_arr.shape[0] < 4:
                        continue
                    pc = pixels_to_pointcloud(
                        depth_image=depth_img,
                        pixels=float_pixel_arr,
                        K=cam_intrinsic,
                    )
                    is_nan_mask = np.isnan(pc).any(axis=1)
                    pc = pc[~is_nan_mask]
                    if pc.shape[0] < 4:
                        continue
                    pick_center_camera = np.mean(pc, axis=0)
                    grasp_position_list.append(pick_center_camera)
            grasp_arr = np.array(grasp_position_list)
            grasp_position_dict[uid] = grasp_arr
        return grasp_position_dict

    def generate_grasp_trajectory(self):
        grasp_positions = np.zeros(shape=(0, 3), dtype=np.float32)
        start_end_id_dict = {}
        for key, value in self.grasp_position_dict.items():
            start_id = grasp_positions.shape[0]
            end_id = start_id + value.shape[0]
            grasp_positions = np.vstack((grasp_positions, value))
            start_end_id_dict[key] = (start_id, end_id)

        # [n_grasp, n_waypoints, dof]
        success, waypoints = self.generate_grasp_waypoints(
            grasp_positions=grasp_positions
        )
        is_valid_arr = success.to("cpu").numpy()

        # do interpolation
        # [n_grasp, n_interp, dof]
        interp_trajectory = interpolate_with_distance_warp(
            trajectory=waypoints, interp_num=100, device=self.robot.device
        )

        # save successful result to dict
        trajectory_dict = {}
        for key, (start_id, end_id) in start_end_id_dict.items():
            trajectory_list = []
            for id in range(start_id, end_id):
                is_valid = is_valid_arr[id]
                if not is_valid:
                    continue
                trajectory_list.append(interp_trajectory[id])
            trajectory_dict[key] = trajectory_list
        return trajectory_dict

    def generate_grasp_waypoints(self, grasp_positions: np.ndarray):
        # move all grasp center near init_xpos
        work_center = self.init_xpos[:3, 3]
        work_center[2] -= 0.4
        grasp_center = grasp_positions.mean(axis=0)
        grasp_posi_translated = grasp_positions - grasp_center + work_center

        # get grasp poses
        rotation = self.init_xpos[:3, :3]
        n_grasp = grasp_posi_translated.shape[0]
        grasp_poses = np.zeros((n_grasp, 4, 4), dtype=np.float32)
        for i in range(n_grasp):
            # TODO: we can add random rotation around z axis
            grasp_poses[i, :3, :3] = rotation
            grasp_poses[i, :3, 3] = grasp_posi_translated[i]
            grasp_poses[i, 3, 3] = 1.0

        # get xpos waypoints
        grasp_poses = torch.tensor(
            grasp_poses, device=self.robot.device, dtype=torch.float32
        )  # [n_grasp, 4, 4]
        approch_poses = grasp_poses.clone()
        approch_poses[:, 2, 3] += 0.1  # approch 10cm above
        batch_init_qpos = self.init_qpos.unsqueeze(0).repeat(1, n_grasp, 1)

        # get qpos
        approach_success, approach_qpos = self.robot.compute_batch_ik(
            pose=approch_poses[None, :, :], name="arm", joint_seed=batch_init_qpos
        )
        grasp_success, grasp_qpos = self.robot.compute_batch_ik(
            pose=grasp_poses[None, :, :], name="arm", joint_seed=approach_qpos
        )
        # pack results
        success = torch.logical_and(approach_success[0], grasp_success[0])
        waypoints = torch.concatenate(
            [
                batch_init_qpos[:, :, None, :],
                approach_qpos[:, :, None, :],
                grasp_qpos[:, :, None, :],
                approach_qpos[:, :, None, :],
                batch_init_qpos[:, :, None, :],
            ],
            dim=2,
        )[0]
        return success, waypoints
